{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining a Language Model\n",
    "\n",
    "Pretraining a language model is a critical step in the development of sophisticated natural language processing (NLP) systems. In the context of modern machine learning, pretraining refers to training a model on a large corpus of text before fine-tuning it for specific downstream tasks. The main goal of pretraining is to enable the model to learn general language patterns, such as grammar, word relationships, context, and meaning. By training on vast amounts of text data, a language model is able to build a comprehensive understanding of language that can be transferred to various specialized tasks with minimal additional data and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Pretraining\n",
    "\n",
    "Pretraining provides several key benefits. First, it significantly reduces the need for task-specific labeled data. By leveraging a large, diverse dataset during pretraining, a model can learn general language representations that are useful across a wide range of NLP tasks. This is particularly beneficial for tasks where labeled data is scarce or difficult to obtain. For example, a model pretrained on millions of documents can perform well in tasks such as sentiment analysis, named entity recognition, or text summarization, even with limited task-specific training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT and GPT: Architectures and Training Approaches\n",
    "\n",
    "Two of the most well-known language models used for pretraining are BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer). Both models use the Transformer architecture, which has become the foundation for many state-of-the-art NLP systems due to its ability to efficiently capture long-range dependencies in text.\n",
    "\n",
    "BERT is designed as a **masked language model (MLM)**. During pretraining, BERT randomly masks some of the words in a sentence and learns to predict these missing words based on the context of the surrounding words. This bidirectional training approach enables BERT to capture the full context of a word, both to the left and right, resulting in richer word representations. For example, in the sentence \"The cat sat on the ___,\" BERT would predict the word \"mat\" based on the context provided by both the words before and after the blank.\n",
    "\n",
    "GPT, on the other hand, is based on a **causal language model (CLM)**. Unlike BERT, GPT is trained to predict the next word in a sequence, conditioned on all previous words. This unidirectional approach helps GPT generate coherent and contextually appropriate text. For example, in the phrase \"The cat sat on the,\" GPT would predict the next word to be \"mat,\" relying only on the words that come before it. GPT's causal nature makes it well-suited for text generation tasks, such as writing articles, answering open-ended questions, or creatingdustries from healthcare to entertainment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Pretrained and Fine-Tuned Language Models\n",
    "\n",
    "The versatility of pretrained language models like BERT and GPT makes them applicable to a wide range of downstream tasks. Fine-tuned models have been used in areas such as:\n",
    "\n",
    "- **Text classification**: Identifying categories or labels for a given piece of text, such as spam detection or sentiment analysis.\n",
    "- **Named entity recognition (NER)**: Identifying and classifying entities (people, organizations, locations, etc.) in a text.\n",
    "- **Question answering**: Finding and providing answers to questions based on a given context or document.\n",
    "- **Text generation**: Automatically generating coherent and contextually appropriate text, useful in content creation, chatbots, and summarization.\n",
    "- **Machine translation**: Translating text from one language to another, which has seen significant improvements with models like GPT.\n",
    "\n",
    "By leveraging the power of pretraining and fine-tuning, language models can be adapted to solve a wide variety of tasks efficiently and accurately, often achieving state-of-the-art results even with relatively small amounts of task-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers -q\n",
    "# !pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "zip_url = \"https://github.com/AsoSoft/AsoSoft-Text-Corpus/raw/master/AsoSoft%20Text%20Corpus%20Small%20Version/AsoSoft Text Corpus- Small Version 1.0 (2018-12-10).zip\"\n",
    "\n",
    "# Define the local filename\n",
    "local_zip_file = \"AsoSoft Text Corpus- Small Version 1.0 (2018-12-10).zip\"\n",
    "\n",
    "# Download the ZIP file\n",
    "print(\"Downloading ZIP file...\")\n",
    "response = requests.get(zip_url)\n",
    "if response.status_code == 200:\n",
    "    with open(local_zip_file, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(f\"Failed to download file: {response.status_code}\")\n",
    "\n",
    "# Unzip the file and rename the desired file\n",
    "print(\"Extracting contents and renaming...\")\n",
    "with zipfile.ZipFile(local_zip_file, \"r\") as zip_ref:\n",
    "    extract_dir = \"data\"  # Directory to extract contents\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "    # Find and rename the text file\n",
    "    for file_name in os.listdir(extract_dir):\n",
    "        if file_name.endswith(\".txt\"):  # Adjust if the file extension is different\n",
    "            old_file_path = os.path.join(extract_dir, file_name)\n",
    "            new_file_path = os.path.join(extract_dir, \"text_data.txt\")\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed '{file_name}' to 'text_data.txt'.\")\n",
    "\n",
    "# Optional: Clean up (delete the ZIP file)\n",
    "os.remove(local_zip_file)\n",
    "print(\"Temporary ZIP file deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Define paths\n",
    "data_file = \"data/text_data.txt\"\n",
    "tokenizer_save_path = \"./bert_tokenizer/\"\n",
    "\n",
    "os.makedirs(tokenizer_save_path, exist_ok=True)\n",
    "\n",
    "# Initialize and train the tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True, strip_accents=True)\n",
    "tokenizer.train(\n",
    "    files=[data_file],\n",
    "    vocab_size=20_000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_model(tokenizer_save_path)\n",
    "print(f\"Tokenizer saved at {tokenizer_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load your text file as a dataset\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": data_file})\n",
    "\n",
    "# Load the trained tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_save_path)\n",
    "\n",
    "\n",
    "# Tokenization and dataset preprocessing\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize input text\n",
    "    encoding = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    return encoding\n",
    "\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_dataset = dataset[\"train\"].map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "# Define the BERT configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=256,\n",
    "    max_position_embeddings=128,\n",
    "    type_vocab_size=2,\n",
    ")\n",
    "\n",
    "# Initialize the BERT model\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "print(f\"{model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-mlm-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=1000,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./bert-mlm-model\")\n",
    "tokenizer.save_pretrained(\"./bert-mlm-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "# Load the model and tokenizer from the saved path\n",
    "model_path = \"./bert-mlm-model\"\n",
    "model = BertForMaskedLM.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Input sentence with a masked token\n",
    "sentence = \"سڵاو [MASK] تۆ.\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get the index of the [MASK] token\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# Get the logits for the masked token(s)\n",
    "mask_token_logits = predictions[0, mask_token_index, :]\n",
    "\n",
    "# Get the top predicted tokens (e.g., top 5)\n",
    "top_k = 5\n",
    "top_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()\n",
    "\n",
    "# Convert token IDs to words\n",
    "predicted_words = [tokenizer.decode([token_id]).strip() for token_id in top_tokens]\n",
    "\n",
    "print(\"Top predictions for the masked token:\")\n",
    "print(predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Or you can use pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_path, tokenizer=model_path)\n",
    "fill_mask(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune your pretrained BERT\n",
    "\n",
    "### Fine-Tuning for Downstream Tasks\n",
    "\n",
    "After pretraining, a language model like BERT or GPT can be fine-tuned for specific downstream tasks. Fine-tuning involves training the model further on task-specific labeled data, adjusting its parameters to optimize performance for that particular task. This allows the model to specialize in areas such as sentiment analysis, named entity recognition, machine translation, or summarization, among others.\n",
    "\n",
    "For example, fine-tuning BERT for a language detection task involves providing the model with a labeled dataset containing sentences and their corresponding sentiment language (English, Arabic, or Kurdish). The model will adjust its parameters to focus on the specific patterns that indicate sentiment.\n",
    "\n",
    "In contrast, fine-tuning GPT for a task like text generation would involve training it on specific text types, such as legal documents or medical reports, so that the model can generate relevant, context-specific content in those domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "dataset = load_dataset(\n",
    "    \"csv\", data_files=\"/kaggle/input/language-detection/language_detection.csv\"\n",
    ")\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "# Access train and test sets\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define label mapping\n",
    "label_mapping = {\"English\": 0, \"Arabic\": 1, \"Kurdish\": 2}\n",
    "\n",
    "# Apply the label mapping to both train and test datasets\n",
    "train_dataset = train_dataset.map(lambda x: {\"label\": label_mapping[x[\"language\"]]})\n",
    "test_dataset = test_dataset.map(lambda x: {\"label\": label_mapping[x[\"language\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./bert-mlm-model\")\n",
    "\n",
    "\n",
    "# Define a tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "\n",
    "# Tokenize the train and test datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unused columns\n",
    "train_dataset = train_dataset.remove_columns([\"sentence\", \"language\"]).with_format(\n",
    "    \"torch\"\n",
    ")\n",
    "test_dataset = test_dataset.remove_columns([\"sentence\", \"language\"]).with_format(\n",
    "    \"torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "num_labels = len(label_mapping)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"./bert-mlm-model\", num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-classification-model\",  # Directory to save model\n",
    "    eval_strategy=\"epoch\",  # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",  # Save model every epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=2,  # Save only the last two checkpoints\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# Define compute_metrics for evaluation\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "\n",
    "    eval_dataset=test_dataset,\n",
    "\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Train and log metrics\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "results = trainer.evaluate()\n",
    "print(\"Test Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "    inputs = {key: val.to(trainer.model.device) for key, val in inputs.items()}\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_label = torch.argmax(outputs.logits, dim=-1).item()\n",
    "\n",
    "    # Map label ID back to language\n",
    "    label = [lang for lang, idx in label_mapping.items() if idx == predicted_label][0]\n",
    "    return label\n",
    "\n",
    "\n",
    "# Example inference\n",
    "print(predict(\"This is an English sentence.\"))\n",
    "print(predict(\"هذه جملة باللغة العربية.\"))\n",
    "print(predict(\"ئەمڕۆ سێ شەمە سەرۆک وەزیرانی فەڕەنسا ڕایگەیاند:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"/kaggle/input/language-detection/language_detection.csv\")\n",
    "\n",
    "# Features and labels\n",
    "X = df[\"sentence\"]\n",
    "y = df[\"language\"]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# One-hot encode target labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Convert text to numerical features using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the RandomForest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test_vectorized)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    clf, X_train_vectorized, y_train_encoded, cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "# Print results\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6304312,
     "sourceId": 10201888,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
