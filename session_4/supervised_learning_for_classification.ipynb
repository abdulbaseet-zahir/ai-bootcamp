{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cover.jpg\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning for Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Classification\n",
    "\n",
    "Classification is a supervised learning task where the model learns to predict discrete class labels from labeled training data. Unlike regression which predicts continuous values, classification predicts categorical outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an email spam classifier. Each email (input) has specific features, such as:\n",
    "\n",
    "- Number of recipients\n",
    "- Presence of specific keywords\n",
    "- Time sent\n",
    "- Links within the email\n",
    "- Senderâ€™s domain\n",
    "\n",
    "The model's task is to classify each email into one of two categories (binary classification):\n",
    "\n",
    "- Spam (1)\n",
    "- Not Spam (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Logistic Regression is a classification algorithm used to predict the probability of a binary outcome, where the target variable can be one of two classes, often labeled as 0 or 1. Despite its name, logistic regression is actually a linear model for classification rather than regression. It works by estimating the probability that an input (or instance) belongs to a specific class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/linear_vs_logistic.png\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Logistic Regression Works\n",
    "\n",
    "1. **Linear Combination of Inputs**: \n",
    "   Logistic regression starts by calculating a weighted sum of the input features. Given a set of input features $X = (x_1, x_2, \\ldots, x_n)$, it computes a linear combination as follows:\n",
    "   \n",
    "   $$z = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n$$\n",
    "   \n",
    "   where $w_0$ is the intercept (bias term), and $( w_1, w_2, \\ldots, w_n )$ are the weights or coefficients associated with each feature.\n",
    "\n",
    "2. **Sigmoid (Logistic) Function**: \n",
    "   The output of the linear combination, $z$, is then passed through the sigmoid function to convert it into a probability value between 0 and 1. The sigmoid function is defined as:\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "   where $e$ is the base of the natural logarithm. The sigmoid function \"squashes\" the output of $z$ to a range between 0 and 1, representing the probability that the input belongs to the positive class (often labeled as 1).\n",
    "\n",
    "3. **Probability Interpretation and Thresholding**:\n",
    "   The output $\\sigma(z)$ can be interpreted as the probability of the instance belonging to the positive class. By default, if this probability is greater than or equal to 0.5, the model assigns the class label 1; otherwise, it assigns 0. However, this threshold can be adjusted based on the problem requirements.\n",
    "\n",
    "4. **Training the Model**:\n",
    "   During training, logistic regression adjusts the weights $w_0, w_1, \\ldots, w_n$ to minimize the difference between the predicted probabilities and the actual class labels. This is often done using maximum likelihood estimation or, more commonly, by minimizing the **binary cross-entropy loss** (log loss) through gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Equation for Logistic Regression\n",
    "\n",
    "The equation for predicting the probability of the positive class $P(y=1|X)$ given input $X$ is:\n",
    "$$\n",
    "P(y=1|X) = \\frac{1}{1 + e^{-(w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n)}}\n",
    "$$\n",
    "\n",
    "In summary, logistic regression is a linear model with a sigmoid transformation applied to make predictions between 0 and 1, representing the probability of belonging to the positive class. This makes it especially useful for binary classification problems where a clear threshold decision is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use\n",
    "- Binary classification problems\n",
    "- When you need probabilistic outcomes\n",
    "- When you want interpretable results\n",
    "- When relationships between features and outcomes are roughly linear\n",
    "- Base model for comparing more complex algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample email spam dataset\n",
    "data = {\n",
    "    \"recipients\": [1, 45, 3, 2, 100, 1, 2, 15, 25, 1],\n",
    "    \"contains_urgent\": [0, 1, 0, 0, 1, 0, 0, 1, 1, 0],\n",
    "    \"links_count\": [1, 15, 2, 1, 20, 0, 2, 10, 12, 1],\n",
    "    \"is_spam\": [0, 1, 0, 0, 1, 0, 0, 1, 1, 0],\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df[[\"recipients\", \"contains_urgent\", \"links_count\"]]\n",
    "y = df[\"is_spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(X_train_scaled, y_train):\n",
    "    print(f\"{i}\\t->\\t{j}\")\n",
    "    print(\"_\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Compare predictions and actual values\n",
    "for i, j, k in zip(X_test_scaled, y_pred, y_test):\n",
    "    print(f\"Input: {i}\\nPredicted: {j}\\nActual: {k}\")\n",
    "    print(\"_\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 100\n",
    "1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Decision Trees are a type of supervised learning algorithm used for both classification and regression tasks. They work by splitting data into subsets based on feature values, creating a tree-like model of decisions. Each internal node of the tree represents a decision based on a specific feature, each branch represents the outcome of that decision, and each leaf node represents a final prediction or outcome. Decision trees are popular because they are easy to understand, interpret, and visualize, and they work well with both numerical and categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/decision_tree_classifier_fruit.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Decision Trees Work\n",
    "\n",
    "1. **Choosing a Feature to Split**:\n",
    "   The decision tree algorithm starts at the root node and selects the feature that best separates the data based on a criterion (often **Gini impurity** or **information gain**). It splits the data into subsets such that each subset contains similar instances in terms of the target variable.\n",
    "\n",
    "2. **Splitting the Data**:\n",
    "   The algorithm evaluates possible splits at each node and selects the one that maximizes the separation between classes (for classification) or minimizes prediction error (for regression). This splitting process is recursive, creating branches in the tree.\n",
    "\n",
    "3. **Stopping Criteria**:\n",
    "   The tree continues splitting the data until one of several stopping criteria is met:\n",
    "   - All data in a node belong to the same class (for classification).\n",
    "   - A maximum depth is reached.\n",
    "   - Further splits do not improve the model significantly.\n",
    "\n",
    "4. **Making Predictions**:\n",
    "   Once the tree is built, predictions are made by traversing the tree from the root node to a leaf node based on the feature values of the input. The label or value in the leaf node is the final prediction for that input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Criteria for Splitting\n",
    "\n",
    "To determine the best splits, decision trees use criteria like **Gini impurity** and **information gain**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Impurity (for Classification)\n",
    "\n",
    "Gini impurity is a measure of how often a randomly chosen element would be incorrectly classified if it was randomly labeled according to the distribution of labels in the subset. For a binary classification, the Gini impurity for a node $( t )$ with two classes (0 and 1) is calculated as:\n",
    "$$\n",
    "\\text{Gini}(t) = 1 - \\sum_{i=1}^C p_i^2\n",
    "$$\n",
    "where $( p_i )$ is the probability of a randomly selected element being classified as class $( i )$, and $( C )$ is the number of classes. A lower Gini impurity indicates a \"purer\" node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain (using Entropy)\n",
    "\n",
    "Another criterion is information gain, which measures the reduction in entropy from a split. Entropy, a measure of disorder, is calculated for a node \\( t \\) as:\n",
    "$\n",
    "\\text{Entropy}(t) = - \\sum_{i=1}^C p_i \\log_2(p_i)\n",
    "$\n",
    "where $( p_i )$ is the probability of class $( i )$ in node $( t )$. Information gain for a split $( S )$ is then calculated as:\n",
    "$$\n",
    "\\text{Information Gain}(S) = \\text{Entropy}(t_{\\text{parent}}) - \\sum_{k=1}^K \\frac{|t_k|}{|t_{\\text{parent}}|} \\text{Entropy}(t_k)\n",
    "$$\n",
    "where $( t_{\\text{parent}} )$ is the original node before the split, $( t_k )$ are the resulting child nodes from the split, and $( |t_k| )$ is the size of each child node.\n",
    "\n",
    "The split with the highest information gain (or lowest Gini impurity) is chosen at each step to build the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Summary, Decision Trees build a model by recursively splitting the dataset based on features that best separate the data, according to criteria like Gini impurity or information gain. The final tree structure can be used for making predictions by following decision paths from the root to the leaf nodes. This structure makes decision trees intuitive and interpretable, though they can sometimes be prone to overfitting, especially if they are allowed to grow too deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use\n",
    "- When you need interpretable results\n",
    "- When you have mixed data types (numerical and categorical)\n",
    "- When you don't need to scale features\n",
    "- When relationships between features and target are non-linear\n",
    "- When you want to capture feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset color, diameter, label of fruits\n",
    "training_data = [\n",
    "    [\"Green\", 3, \"Apple\"],\n",
    "    [\"Yellow\", 3, \"Apple\"],\n",
    "    [\"Red\", 1, \"Grape\"],\n",
    "    [\"Red\", 1, \"Grape\"],\n",
    "    [\"Yellow\", 3, \"Lemon\"],\n",
    "    [\"Red\", 3, \"Apple\"],\n",
    "    [\"Green\", 3, \"Pear\"],\n",
    "    [\"Yellow\", 2, \"Pear\"],\n",
    "    [\"Purple\", 1, \"Grape\"],\n",
    "    [\"Green\", 1, \"Grape\"],\n",
    "    [\"Yellow\", 3, \"Lemon\"],\n",
    "    [\"Green\", 2, \"Lime\"],\n",
    "    [\"Yellow\", 2, \"Lemon\"],\n",
    "    [\"Red\", 2, \"Plum\"],\n",
    "    [\"Purple\", 2, \"Plum\"],\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(training_data, columns=[\"color\", \"diameter\", \"label\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize label encoders for features\n",
    "color_encoder = LabelEncoder()\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode features and labels from strings to integers to be used in the model\n",
    "df[\"color\"] = color_encoder.fit_transform(df[\"color\"])\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df[[\"color\", \"diameter\"]]\n",
    "y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "dt_clf = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_dt = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance = pd.DataFrame(\n",
    "    {\"feature\": X.columns, \"importance\": dt_clf.feature_importances_}\n",
    ")\n",
    "\n",
    "importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Random Forests is an ensemble learning method used for classification and regression tasks. It operates by building a collection (or \"forest\") of decision trees and combining their predictions to produce a more robust, accurate model. Random forests improve on individual decision trees by reducing variance, making the model less prone to overfitting and more capable of handling complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/random_forest_classifier_fruit.jpg\" alt=\"ML Workflow Diagram\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Random Forests Work\n",
    "\n",
    "1. **Creating Multiple Decision Trees**:\n",
    "   A random forest creates multiple decision trees using different subsets of the training data. For each tree, a random sample of the dataset is selected with replacement, a process called **bootstrap sampling**. This means some instances may appear more than once in a tree's training set, while others may be left out.\n",
    "\n",
    "2. **Feature Randomness**:\n",
    "   In addition to sampling data, random forests also introduce randomness in feature selection. When splitting nodes, each tree considers only a random subset of features rather than all features. This \"feature randomness\" helps to make the individual trees less correlated with each other, reducing overfitting.\n",
    "\n",
    "3. **Building the Trees**:\n",
    "   Each decision tree is built independently using the selected subset of data and features. The trees are grown to their maximum depth (or another stopping criterion) without pruning, which allows each tree to capture patterns in the data.\n",
    "\n",
    "4. **Combining the Predictions**:\n",
    "   Once all trees are built, the random forest combines their predictions. For **classification tasks**, it uses **majority voting**: each tree votes for a class, and the class with the most votes is the final prediction. For **regression tasks**, it averages the predictions of all trees.\n",
    "\n",
    "5. **Out-of-Bag (OOB) Error**:\n",
    "   An additional advantage of random forests is that it can estimate the error without needing a separate validation set. Since each tree is trained on a bootstrap sample, about one-third of the instances are left out (called \"out-of-bag\" data). These OOB instances are used to evaluate the model's accuracy, providing an unbiased error estimate.\n",
    "\n",
    "### Mathematical Formulation of Random Forests\n",
    "\n",
    "1. **Bootstrap Sampling**:\n",
    "   Let the dataset have $( N )$ instances. For each tree $( T_i )$, draw a random sample of size $( N )$ with replacement. This bootstrap sample is used to train tree $( T_i )$.\n",
    "\n",
    "2. **Random Feature Selection**:\n",
    "   At each node in the tree, a subset of $( m )$ features is randomly selected from the total $( p )$ features (where typically $( m \\approx \\sqrt{p} )$ for classification and $( m \\approx \\frac{p}{3} )$ for regression). The feature that best splits the data is chosen from this subset.\n",
    "\n",
    "3. **Combining Predictions**:\n",
    "   - **For Classification**: If there are $( B )$ trees, each tree $( T_i )$ provides a class prediction $( y_i )$. The random forest's final prediction $( \\hat{y} )$ is determined by majority voting:\n",
    "     $$\n",
    "     \\hat{y} = \\text{mode}(y_1, y_2, \\ldots, y_B)\n",
    "     $$\n",
    "   - **For Regression**: Each tree $( T_i $) produces a predicted value $( \\hat{y}_i )$. The final prediction $( \\hat{y} $) of the random forest is the average of all tree predictions:\n",
    "     $$\n",
    "     \\hat{y} = \\frac{1}{B} \\sum_{i=1}^B \\hat{y}_i\n",
    "     $$\n",
    "\n",
    "4. **Out-of-Bag Error (OOB Error)**:\n",
    "   For each instance in the dataset, predictions are made only by the trees that did not use it in their bootstrap sample. The OOB error is calculated as the average error on these OOB predictions, which serves as an unbiased estimate of the model's performance.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Random Forests combine the predictions of multiple decision trees trained on different subsets of data and features. By introducing randomness in data sampling and feature selection, random forests produce a more generalized and robust model than individual decision trees, reducing the risk of overfitting. This ensemble method works well with large datasets and is highly effective for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.randint(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Compare predictions and actual values\n",
    "for i, j, k in zip(X_test.values, y_pred, y_test):\n",
    "    print(\n",
    "        f\"Input: Color: {i[0]} ({color_encoder.inverse_transform([i[0]])[0]}), Diameter: {i[1]}\"\n",
    "    )\n",
    "    print(f\"Predicted: {j} ({label_encoder.inverse_transform([j])[0]})\")\n",
    "    print(f\"Actual: {k} ({label_encoder.inverse_transform([k])[0]})\")\n",
    "\n",
    "    print(\"_\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Each model has its strengths:\n",
    "\n",
    "1. **Logistic Regression**:\n",
    "   - Simple and interpretable\n",
    "   - Provides probability scores\n",
    "   - Works well with linear relationships\n",
    "   - Fast to train and predict\n",
    "   - Requires feature scaling\n",
    "\n",
    "2. **Decision Trees**:\n",
    "   - Highly interpretable\n",
    "   - Handles non-linear relationships\n",
    "   - No scaling required\n",
    "   - Can capture feature interactions\n",
    "   - Prone to overfitting\n",
    "\n",
    "3. **Random Forests**:\n",
    "   - Generally better accuracy than single trees\n",
    "   - Less prone to overfitting\n",
    "   - Provides reliable feature importance\n",
    "   - Handles non-linear relationships\n",
    "   - Less interpretable than single trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical: Real world Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the Dataset**\n",
    "\n",
    "**Context**  \n",
    "This [dataset](https://www.kaggle.com/code/megoooo/heart-disease-logistecregression) was created in 1988 and combines information from four sources: Cleveland, Hungary, Switzerland, and Long Beach V. It includes 76 attributes, but studies typically use only 14 of them. The \"target\" field shows if the patient has heart disease, with 0 meaning no disease and 1 meaning disease.\n",
    "\n",
    "**Content**  \n",
    "Hereâ€™s a list of the 14 key attributes:\n",
    "\n",
    "- Age\n",
    "- Sex\n",
    "- Chest pain type (4 types)\n",
    "- Resting blood pressure\n",
    "- Serum cholesterol level (mg/dl)\n",
    "- Fasting blood sugar > 120 mg/dl\n",
    "- Resting electrocardiographic results (0, 1, or 2)\n",
    "- Maximum heart rate achieved\n",
    "- Exercise-induced angina\n",
    "- Oldpeak (ST depression caused by exercise vs. rest)\n",
    "- Slope of the peak exercise ST segment\n",
    "- Number of major vessels (0-3) colored by fluoroscopy\n",
    "- Thalassemia type (0 = normal, 1 = fixed defect, 2 = reversible defect)\n",
    "\n",
    "Patient names and social security numbers have been replaced with anonymous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/heart.csv\")\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Target variable distribution\n",
    "sns.countplot(x=\"target\", data=data)\n",
    "plt.title(\"Distribution of Target (0 = No Heart Disease, 1 = Heart Disease)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_features = [\"cp\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\"]\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "X_selected = data[high_corr_features]\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_selected = data\n",
    "# X_selected = X_selected.drop(\"target\", axis=1)\n",
    "# y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "X_scaled = scaler.fit_transform(X_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_scaled, columns=X_selected.columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(\"\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "    yticklabels=[\"Actual Negative\", \"Actual Positive\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix visualizes the performance of the binary classification (Logistic Regression) model. The matrix is divided into four cells that represent different prediction outcomes:\n",
    "\n",
    "- **True Negatives (Top-left)**: 74 instances were correctly identified as negative (i.e., the model predicted \"negative,\" and the actual label was also \"negative\").\n",
    "- **False Positives (Top-right)**: 28 instances were incorrectly predicted as positive when they were actually negative.\n",
    "- **False Negatives (Bottom-left)**: 13 instances were incorrectly predicted as negative when they were actually positive.\n",
    "- **True Positives (Bottom-right)**: 90 instances were correctly identified as positive.\n",
    "\n",
    "This matrix shows that the model correctly identified 74 negatives and 90 positives, while it misclassified 28 negatives as positives and 13 positives as negatives. \n",
    "\n",
    "This information can be used to calculate key metrics like accuracy, precision, recall, and F1-score, helping to assess the model's effectiveness in identifying positive and negative cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
