{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cover.jpg\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer architecture, introduced in the paper *\"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\"* by Vaswani et al., revolutionized natural language processing by eliminating reliance on recurrence or convolutional layers, common in earlier models. Instead, it uses a mechanism called *self-attention* to compute the dependencies between all input tokens simultaneously, enabling efficient parallel processing and capturing long-range relationships in sequences. \n",
    "\n",
    "The architecture consists of an encoder-decoder structure, where the encoder processes input sequences and the decoder generates output sequences, both leveraging multi-head attention and feed-forward layers. This design achieved state-of-the-art results in machine translation and laid the foundation for subsequent advancements in language models like BERT and GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/transformer architecture.webp\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/transformer explained.jpg\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "The Transformer architecture, unlike RNNs, processes input sequences in parallel, making it inherently unaware of the sequential order of tokens. To address this, *positional encoding* is introduced, enabling the model to incorporate information about the position of each token in a sequence. This is achieved by embedding positional information directly into the input representations using a combination of sine and cosine functions. These functions encode position in a continuous and differentiable manner, allowing the model to capture relative and absolute positions of tokens effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/positional encoding.png\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinusoidal Positional Encoding\n",
    "The sine and cosine functions are chosen for their periodic nature, ensuring unique positional representations for each token. For a given position $ pos $ and model dimension $ i $, the positional encoding is calculated as:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right), \\quad \n",
    "PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ d_{\\text{model}} $ is the model dimension.\n",
    "- $ i $ alternates between even and odd indices in the embedding vector.\n",
    "\n",
    "The alternating sine and cosine patterns ensure a distinct representation for each position while preserving the geometric relationships between positions. These positional encodings are added directly to the token embeddings, enabling the Transformer to incorporate positional information during training and inference. \n",
    "\n",
    "In the provided implementation, this encoding is computed using PyTorch. The sine function is applied to even indices, and the cosine function to odd indices. The resulting positional encodings are stored as a buffer and added to the input tensor during the forward pass, allowing the model to leverage position-awareness without requiring additional trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(0, 10, dtype=torch.float).unsqueeze(1)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term = torch.exp(torch.arange(0, 8, 2).float() * (-math.log(10000.0) / 8))\n",
    "div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position * div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sin(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_length: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension and register as buffer\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:, : x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Positional Encoding\n",
    "\n",
    "The `SimplePositionalEncoding` class provides an alternative approach to encoding positional information in sequence models. Instead of using fixed sine and cosine functions, this method introduces a learnable positional embedding. A trainable parameter matrix, `positional_embedding`, is initialized randomly and has dimensions corresponding to the maximum sequence length (`max_seq_length`) and model dimension (`d_model`).  \n",
    "\n",
    "During the forward pass, the positional embeddings for the relevant sequence length are added directly to the input tensor, enabling the model to learn optimal positional encodings during training. This approach is simpler and more flexible but introduces additional parameters compared to fixed encodings like sinusoidal functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altarnative simple positional encoding\n",
    "class SimplePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_length: int = 5000):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(max_seq_length, d_model))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.positional_embedding[: x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head attention is a core component of the Transformer architecture, enabling the model to focus on different parts of the input sequence simultaneously. It extends the self-attention mechanism by dividing the model's attention capacity into multiple \"heads,\" each learning unique aspects of the input relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query (Q), Key (K), and Value (V)  \n",
    "The inputs to multi-head attention are transformed into three representations:  \n",
    "- **Query (Q):** Represents the \"what to attend to\" aspect of each token.  \n",
    "- **Key (K):** Encodes \"where to attend\" based on the relationships between tokens.  \n",
    "- **Value (V):** Contains the actual information to be aggregated from the attended tokens.  \n",
    "\n",
    "These representations are computed using learned linear transformations:  \n",
    "$$\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "$$ \n",
    "where $ X $ is the input sequence and $ W_Q, W_K, W_V $ are trainable projection matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Mechanism  \n",
    "The input embeddings are split into $ h $ heads (determined by `num_heads`), each operating on a subset of the total embedding dimension $ d_{\\text{model}} $. For each head:  \n",
    "1. **Scaled Dot-Product Attention** is computed:  \n",
    "   $$\n",
    "   \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "   $$ \n",
    "   Here, $ d_k = d_{\\text{model}} / h $, and the scaling factor $ \\sqrt{d_k} $ prevents overly large attention scores that could lead to small gradients.  \n",
    "2. Outputs from all heads are concatenated and projected back into the original dimension $ d_{\\text{model}} $ using a linear layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/multi-head-attention.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention  \n",
    "The scaled dot-product attention computes attention scores between the query and key matrices, determines the relevance of each token, and uses these scores to aggregate the value matrix. The result is a context-aware representation of the input sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/scaled dot-product attention.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Multi-Head Attention  \n",
    "Multi-head attention allows the model to jointly attend to information from different positions and capture diverse relationships in the input sequence. This enhances the model's ability to understand complex dependencies and patterns across tokens.\n",
    "\n",
    "The provided implementation follows this structure, defining separate linear layers for $ Q $, $ K $, and $ V $, and applying scaled dot-product attention across multiple heads. Finally, the results are combined and projected back to the original embedding size, making the mechanism both powerful and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear layers for Q, K, V projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(\n",
    "        self,\n",
    "        Q: torch.Tensor,\n",
    "        K: torch.Tensor,\n",
    "        V: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        return torch.matmul(attention_weights, V)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear projections and reshape\n",
    "        Q = (\n",
    "            self.W_q(query)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = (\n",
    "            self.W_v(value)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        # Apply attention\n",
    "        x = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Reshape and apply final linear layer\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.W_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Networks\n",
    "\n",
    "In the Transformer architecture, the feed-forward network (FFN) is a fully connected sublayer applied independently to each position in the sequence. It consists of two linear transformations with a non-linear activation (typically ReLU) in between. The FFN enhances the model's capacity to learn complex mappings between input and output embeddings.  \n",
    "\n",
    "The process involves:  \n",
    "1. Expanding the dimensionality of the input from $ d_{\\text{model}} $ to $ d_{\\text{ff}} $ using the first linear layer.  \n",
    "2. Applying a non-linear activation function (ReLU) to introduce non-linearity.  \n",
    "3. Reducing the dimensionality back to $ d_{\\text{model}} $ using the second linear layer.  \n",
    "4. Using dropout for regularization, improving generalization.  \n",
    "\n",
    "### Benefits of the Feed-Forward Network  \n",
    "- **Position-wise Computation:** The FFN operates on each token independently, allowing parallel computation across all sequence positions.  \n",
    "- **Increased Model Capacity:** The intermediate expansion to $ d_{\\text{ff}} $ provides a higher capacity to model complex transformations, improving expressiveness.  \n",
    "- **Non-linearity:** The ReLU activation captures non-linear patterns that cannot be modeled by attention mechanisms alone.  \n",
    "\n",
    "This implementation reflects the standard FFN, efficiently balancing computational complexity and modeling power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Layer\n",
    "\n",
    "The encoder layer is a fundamental building block of the Transformer, designed to process input sequences and generate contextualized representations. Each encoder layer consists of two key subcomponents:  \n",
    "\n",
    "1. **Self-Attention Block:**  \n",
    "   - Employs multi-head attention to allow each token to attend to all other tokens in the input sequence, capturing relationships and dependencies.  \n",
    "   - Outputs are normalized (via LayerNorm) and combined with residual connections to facilitate stable and efficient training.  \n",
    "\n",
    "2. **Feed-Forward Network:**  \n",
    "   - A position-wise feed-forward network enhances the capacity of the model to capture complex transformations.  \n",
    "   - Includes normalization and residual connections, similar to the self-attention block.  \n",
    "\n",
    "### Workflow:  \n",
    "1. Input passes through the self-attention block, with optional masking to focus on specific tokens.  \n",
    "2. The attention output is added to the original input (residual connection), normalized, and passed to the feed-forward network.  \n",
    "3. The feed-forward output undergoes another residual connection and normalization step.  \n",
    "\n",
    "The encoder layer effectively learns rich representations of the input sequence by combining attention mechanisms and non-linear transformations, enabling robust feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Self attention block\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Layer \n",
    "\n",
    "The decoder layer processes target sequences while integrating information from the encoder's output to generate predictions. It consists of three main components:  \n",
    "\n",
    "1. **Self-Attention Block:**  \n",
    "   - Applies multi-head self-attention to the target sequence, with masking to prevent tokens from attending to future positions (causal masking).  \n",
    "\n",
    "2. **Cross-Attention Block:**  \n",
    "   - Employs multi-head attention where the queries come from the decoder, and keys and values are derived from the encoder's output, allowing the decoder to attend to relevant parts of the source sequence.  \n",
    "\n",
    "3. **Feed-Forward Network:**  \n",
    "   - A position-wise feed-forward network applies non-linear transformations, enhancing representation capabilities.  \n",
    "\n",
    "### Workflow:  \n",
    "1. Input goes through the self-attention block, ensuring the model focuses only on prior positions in the sequence.  \n",
    "2. Cross-attention combines the decoder's self-attention output with the encoder's output for contextual understanding.  \n",
    "3. The result is passed through the feed-forward network for further processing.  \n",
    "\n",
    "Each subcomponent is wrapped with residual connections, LayerNorm, and dropout, ensuring efficient training and preventing vanishing gradients. The decoder layer excels at learning dependencies between target and source sequences for accurate generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        enc_output: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None,\n",
    "        src_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # Self attention block\n",
    "        self_attn_output = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "\n",
    "        # Cross attention block\n",
    "        cross_attn_output = self.cross_attention(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "\n",
    "        # Feed forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer \n",
    "\n",
    "The `Transformer` class implements the full Transformer architecture, combining encoder and decoder components to process sequences for tasks like machine translation and text generation.  \n",
    "\n",
    "### Key Components:  \n",
    "1. **Embedding and Positional Encoding:**  \n",
    "   - Inputs and target sequences are embedded into continuous vectors and augmented with positional encodings to include token position information.  \n",
    "\n",
    "2. **Encoder:**  \n",
    "   - A stack of encoder layers processes the source sequence, generating context-rich representations by applying self-attention and feed-forward sublayers.  \n",
    "\n",
    "3. **Decoder:**  \n",
    "   - A stack of decoder layers combines target sequence information with the encoder's output. It uses self-attention to focus on prior tokens and cross-attention to incorporate relevant context from the source sequence.  \n",
    "\n",
    "4. **Final Output Layer:**  \n",
    "   - A linear layer maps the decoder's output to the target vocabulary space, producing logits for each token prediction.  \n",
    "\n",
    "### Workflow:  \n",
    "1. Input sequences are embedded and passed through the encoder layers, with masks applied to handle padding.  \n",
    "2. Target sequences are similarly embedded and processed through decoder layers, with causal masks to prevent access to future tokens.  \n",
    "3. The decoder output is transformed into predictions via the final linear layer.  \n",
    "\n",
    "### Benefits:  \n",
    "The `Transformer` model is highly parallelizable, handles long-range dependencies effectively, and achieves state-of-the-art results in sequence-to-sequence tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        d_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 6,\n",
    "        d_ff: int = 2048,\n",
    "        max_seq_length: int = 5000,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Create encoder and decoder layers\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, src: torch.Tensor, tgt: torch.Tensor) -> tuple:\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (\n",
    "            1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)\n",
    "        ).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        src_mask, tgt_mask = self.create_mask(src, tgt)\n",
    "\n",
    "        # Encoder\n",
    "        src_embedded = self.dropout(\n",
    "            self.positional_encoding(self.encoder_embedding(src))\n",
    "        )\n",
    "        enc_output = src_embedded\n",
    "\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        # Decoder\n",
    "        tgt_embedded = self.dropout(\n",
    "            self.positional_encoding(self.decoder_embedding(tgt))\n",
    "        )\n",
    "        dec_output = tgt_embedded\n",
    "\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, tgt_mask, src_mask)\n",
    "\n",
    "        output = self.final_layer(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Dataset\n",
    "\n",
    "### PyTorch Dataset\n",
    "\n",
    "PyTorch's `Dataset` class provides a framework for handling and preprocessing data for machine learning models. It allows custom datasets to be created with methods to define how data is loaded and processed, making it compatible with PyTorch's data loaders for efficient batch processing.\n",
    "\n",
    "### `TranslationDataset` Class:  \n",
    "This custom dataset is designed for machine translation tasks, processing paired source-target text data.\n",
    "\n",
    "#### Key Features:  \n",
    "1. **Initialization (`__init__`):**  \n",
    "   - Takes a dataset of source and target text pairs, along with vocabularies for token-to-index mapping.  \n",
    "   - Supports sequence truncation and padding up to a specified `max_length`.  \n",
    "\n",
    "2. **Length (`__len__`):**  \n",
    "   - Returns the total number of text pairs in the dataset, enabling iteration.\n",
    "\n",
    "3. **Item Retrieval (`__getitem__`):**  \n",
    "   - Converts source (`src_text`) and target (`tgt_text`) sequences into token indices using the provided vocabularies.  \n",
    "   - Truncates sequences to `max_length` and pads shorter ones with zeros to ensure uniform length.  \n",
    "   - Returns a dictionary containing tokenized and padded sequences as PyTorch tensors.\n",
    "\n",
    "### Benefits:  \n",
    "- **Preprocessing Flexibility:** Simplifies text-to-index conversion and padding, making the dataset ready for use in models.  \n",
    "- **Compatibility:** Works seamlessly with PyTorch's `DataLoader` for batch processing, shuffling, and parallel data loading.  \n",
    "- **Customizability:** Can be extended to include additional features like masking or special token handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, src_vocab, tgt_vocab, max_length=100):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text, tgt_text = self.data[idx]\n",
    "\n",
    "        # Convert text to indices\n",
    "        src_indices = [self.src_vocab[token] for token in src_text.split()][\n",
    "            : self.max_length\n",
    "        ]\n",
    "        tgt_indices = [self.tgt_vocab[token] for token in tgt_text.split()][\n",
    "            : self.max_length\n",
    "        ]\n",
    "\n",
    "        # Pad sequences\n",
    "        src_indices = src_indices + [0] * (self.max_length - len(src_indices))\n",
    "        tgt_indices = tgt_indices + [0] * (self.max_length - len(tgt_indices))\n",
    "\n",
    "        return {\"src\": torch.tensor(src_indices), \"tgt\": torch.tensor(tgt_indices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"hello world\", \"hallo welt\"),\n",
    "    (\"goodbye\", \"auf wiedersehen\"),\n",
    "    (\"how are you?\", \"wie geht es dir?\"),\n",
    "]\n",
    "\n",
    "src_vocab = {\"hello\": 10, \"world\": 1, \"goodbye\": 2, \"how\": 3, \"are\": 4, \"you?\": 5}\n",
    "tgt_vocab = {\"hallo\": 10, \"welt\": 1, \"auf\": 2, \"wiedersehen\": 3, \"wie\": 4, \"geht\": 5}\n",
    "\n",
    "dataset = TranslationDataset(data, src_vocab, tgt_vocab)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            src = batch[\"src\"].to(device)\n",
    "            tgt = batch[\"tgt\"].to(device)\n",
    "\n",
    "            # Create target input and output\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input)\n",
    "\n",
    "            # Reshape output and target for loss calculation\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def train():\n",
    "    train_iter = [\n",
    "        (\"hello world\", \"hallo welt\"),\n",
    "        (\"goodbye\", \"auf wiedersehen\"),\n",
    "        (\"how are you?\", \"wie geht es dir?\"),\n",
    "        (\"I'm fine\", \"ich bin gut\"),\n",
    "        (\"what's your name?\", \"wie heißt du?\"),\n",
    "        (\"my name is John\", \"ich heiße John\"),\n",
    "        (\"nice to meet you\", \"freut mich\"),\n",
    "        (\"where are you from?\", \"woher kommst du?\"),\n",
    "        (\"I'm from Germany\", \"ich komme aus Deutschland\"),\n",
    "        (\"do you speak English?\", \"sprichst du Englisch?\"),\n",
    "        (\"yes, a little\", \"ja, ein bisschen\"),\n",
    "        (\"what time is it?\", \"wie spät ist es?\"),\n",
    "        (\"it's three o'clock\", \"es ist drei Uhr\"),\n",
    "        (\"I'm hungry\", \"ich habe Hunger\"),\n",
    "        (\"let's eat something\", \"lass uns etwas essen\"),\n",
    "        (\"the food is delicious\", \"das Essen ist lecker\"),\n",
    "        (\"thank you very much\", \"vielen Dank\"),\n",
    "        (\"you're welcome\", \"bitte schön\"),\n",
    "        (\"see you tomorrow\", \"bis morgen\"),\n",
    "        (\"have a good day\", \"einen schönen Tag\"),\n",
    "        (\"I love reading\", \"ich liebe es zu lesen\"),\n",
    "        (\"what's the weather like?\", \"wie ist das Wetter?\"),\n",
    "        (\"it's raining\", \"es regnet\"),\n",
    "        (\"the sun is shining\", \"die Sonne scheint\"),\n",
    "        (\"I need help\", \"ich brauche Hilfe\"),\n",
    "        (\"can you help me?\", \"kannst du mir helfen?\"),\n",
    "        (\"of course\", \"natürlich\"),\n",
    "        (\"excuse me\", \"entschuldigung\"),\n",
    "        (\"I'm sorry\", \"es tut mir leid\"),\n",
    "        (\"no problem\", \"kein Problem\"),\n",
    "        (\"how much is this?\", \"wie viel kostet das?\"),\n",
    "        (\"that's expensive\", \"das ist teuer\"),\n",
    "        (\"I'll take it\", \"ich nehme es\"),\n",
    "        (\"where is the bathroom?\", \"wo ist die Toilette?\"),\n",
    "        (\"turn left\", \"links abbiegen\"),\n",
    "        (\"turn right\", \"rechts abbiegen\"),\n",
    "        (\"go straight ahead\", \"geradeaus gehen\"),\n",
    "        (\"I'm lost\", \"ich habe mich verlaufen\"),\n",
    "        (\"can you show me the way?\", \"können Sie mir den Weg zeigen?\"),\n",
    "        (\"I don't understand\", \"ich verstehe nicht\"),\n",
    "        (\"please speak slowly\", \"bitte sprechen Sie langsam\"),\n",
    "        (\"could you repeat that?\", \"können Sie das wiederholen?\"),\n",
    "        (\"what does this mean?\", \"was bedeutet das?\"),\n",
    "        (\"I like this\", \"das gefällt mir\"),\n",
    "        (\"that's interesting\", \"das ist interessant\"),\n",
    "        (\"I agree\", \"ich stimme zu\"),\n",
    "        (\"I disagree\", \"ich stimme nicht zu\"),\n",
    "        (\"good morning\", \"guten Morgen\"),\n",
    "        (\"good evening\", \"guten Abend\"),\n",
    "        (\"good night\", \"gute Nacht\"),\n",
    "    ]\n",
    "\n",
    "    src_vocab = Counter()\n",
    "    tgt_vocab = Counter()\n",
    "\n",
    "    for src, tgt in train_iter:\n",
    "        src_vocab.update(src.split())\n",
    "        tgt_vocab.update(tgt.split())\n",
    "\n",
    "    # Create vocabulary dictionaries\n",
    "    src_vocab = {word: idx + 1 for idx, (word, _) in enumerate(src_vocab.most_common())}\n",
    "    tgt_vocab = {word: idx + 1 for idx, (word, _) in enumerate(tgt_vocab.most_common())}\n",
    "    src_vocab[\"<pad>\"] = 0\n",
    "    tgt_vocab[\"<pad>\"] = 0\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TranslationDataset(list(train_iter), src_vocab, tgt_vocab)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Transformer(\n",
    "        src_vocab_size=len(src_vocab),\n",
    "        tgt_vocab_size=len(tgt_vocab),\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        d_ff=2048,\n",
    "    ).to(device)\n",
    "\n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(model, train_loader, criterion, optimizer, device, 10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some valuable external resources to deepen your understanding of Transformers:\n",
    "\n",
    "- [**Attention Is All You Need**](https://arxiv.org/abs/1706.03762) – The foundational paper that introduced the Transformer model.\n",
    "- [**The Illustrated Transformer**](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar – A visually intuitive explanation of the Transformer architecture.\n",
    "- [**PyTorch Transformers from Scratch**](https://www.youtube.com/watch?v=U0s0f995w14&t=729s) by Aladdin Persson – A practical video guide on building Transformers in PyTorch.\n",
    "- [**The Annotated Transformer**](http://nlp.seas.harvard.edu/annotated-transformer/) by Sasha Rush – A detailed, step-by-step explanation of the Transformer model's code.\n",
    "- [**Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch**](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) by Sebastian Raschka – A deep dive into the self-attention mechanism and how to implement it from scratch.\n",
    "- [**Transformers from Scratch**](https://peterbloem.nl/blog/transformers) by Peter Bloem – An insightful blog post on understanding and implementing Transformers from the ground up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1, 3, 1, 5)\n",
    "print(x.shape)  # Output: torch.Size([1, 3, 1, 5])\n",
    "\n",
    "y = x.squeeze()\n",
    "print(y.shape)  # Output: torch.Size([3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
