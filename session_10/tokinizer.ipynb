{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cover.jpg\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Deep learning tokenization refers to the process of converting text into smaller units, called tokens, to make it suitable for neural networks. It starts with simple methods like space-based splitting (dividing text by spaces into words) and progresses to more advanced techniques such as WordPiece and Byte Pair Encoding (BPE). WordPiece breaks words into subword units, allowing the model to handle rare or unknown words effectively. BPE uses frequency-based merging of characters to create subword units, improving efficiency in handling large vocabularies and complex languages. These techniques help models like transformers understand and process text better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/tokenization.svg\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset for tokenizer training by ensuring proper formatting and creating a list of file paths if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download the dataset uncomment the following line\n",
    "# ! python download_asosoft_small_text_corpus.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"data/text_data.txt\"  # a text file with one article per line\n",
    "# Read the input file and split by newlines\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = f.read().split(\"\\n\")\n",
    "\n",
    "# Remove empty lines and create temporary files\n",
    "articles = [article.strip() for article in articles if article.strip()]\n",
    "\n",
    "# Create a temporary directory for individual article files\n",
    "os.makedirs(\"temp_articles\", exist_ok=True)\n",
    "\n",
    "# Write each article to a separate file\n",
    "files = []\n",
    "for idx, article in enumerate(articles):\n",
    "    file_path = f\"temp_articles/article_{idx}.txt\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(article)\n",
    "    files.append(file_path)\n",
    "\n",
    "print(f\"Created {len(files)} temporary files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a BERT WordPiece tokenizer on the provided dataset.\n",
    "\n",
    "- files: List of file paths containing the training texts\n",
    "- vocab_size: Size of the final vocabulary\n",
    "- min_frequency: Minimum frequency for a token to be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "min_frequency = 2\n",
    "\n",
    "# Initialize a BERT WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True, handle_chinese_chars=True, strip_accents=False, lowercase=False\n",
    ")\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(\n",
    "    files=files,\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    limit_alphabet=1000,\n",
    "    wordpieces_prefix=\"##\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"custom_tokenizer\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "tokenizer.save(\"custom_tokenizer/tokenizer.json\")\n",
    "\n",
    "# Convert to PreTrainedTokenizerFast\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=f\"{output_dir}/tokenizer.json\",\n",
    "    # Add BERT-specific parameters\n",
    "    bos_token=\"[CLS]\",\n",
    "    eos_token=\"[SEP]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# Save the fast tokenizer\n",
    "fast_tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained tokenizer on a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"This is a sample text to test our new BERT tokenizer.\"\n",
    "print(f\"Original text: {test_text}\")\n",
    "\n",
    "# Encode the text\n",
    "ids = tokenizer.encode(test_text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "# Print the results\n",
    "print(f\"Encoded tokens: {tokens}\")\n",
    "print(f\"Token IDs: {ids}\")\n",
    "\n",
    "# Decode the tokens\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "for file in files:\n",
    "    os.remove(file)\n",
    "os.rmdir(\"temp_articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
