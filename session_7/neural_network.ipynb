{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Fundamentals\n",
    "This session will guide you through building, training, and evaluating a neural network using the MNIST dataset in PyTorch. We'll explore key concepts such as data preprocessing, feedforward passes, activation functions, loss functions, backpropagation, and model evaluation. Finally, we'll save and load the trained model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset contains images of handwritten digits (0-9) and their corresponding labels. It includes:\n",
    "- **Training Set**: 60,000 images\n",
    "- **Test Set**: 10,000 images  \n",
    "Each image is grayscale, with a resolution of **28x28 pixels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "Normalization scales pixel values to a smaller range (e.g., [0, 1] or [-1, 1]), helping the model converge faster and avoid large gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations: normalize pixel values\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize((0.5,), (0.5,)),  # Normalize pixel values to [-1, 1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transforms.Compose()` is used to define a pipeline of transformations applied to each image in the dataset.\n",
    "\n",
    "`transforms.ToTensor()`\n",
    "- Converts an image (PIL Image or NumPy array) to a PyTorch tensor.\n",
    "- The pixel values of the image are scaled to the range $([0, 1])$ by dividing each pixel value by 255. For example, a pixel value of $(255)$ becomes $(1.0)$, and $(0)$ remains $(0.0)$.\n",
    "\n",
    "`transforms.Normalize((0.5,), (0.5,))`\n",
    "- Normalizes the tensor values to have a mean of $(0)$ and a standard deviation of $(1)$.\n",
    "- Formula for normalization:\n",
    "  $$\n",
    "  \\text{Normalized Value} = \\frac{\\text{Value} - \\text{Mean}}{\\text{Standard Deviation}}\n",
    "  $$\n",
    "  Here, the mean is $(0.5)$ and the standard deviation is $(0.5)$, so:\n",
    "  $$\n",
    "  \\text{Normalized Value} = \\frac{\\text{Value} - 0.5}{0.5} = 2 \\cdot (\\text{Value} - 0.5)\n",
    "  $$\n",
    "  This scales pixel values from $([0, 1])$ (after `ToTensor`) to $([-1, 1])$:\n",
    "  - $(0)$ becomes $(-1)$\n",
    "  - $(0.5)$ becomes $(0)$\n",
    "  - $(1)$ becomes $(1)$\n",
    "\n",
    "By normalizing the pixel values, the model can learn faster and better because all inputs have a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"data\", train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"data\", train=False, transform=transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datasets.MNIST` class downloads and loads the MNIST dataset with the transformations applied. The `train=True` flag indicates the training set, while `train=False` loads the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.utils.data.DataLoader` wraps the dataset, allowing us to:\n",
    "- **Batch** the data (`batch_size=64`).\n",
    "- Shuffle the training data (`shuffle=True` for randomness).\n",
    "- Sequentially load the test data (`shuffle=False` for evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing some samples helps us understand the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Plot the first 6 images\n",
    "fig, axes = plt.subplots(1, 6, figsize=(10, 3))\n",
    "for i in range(6):\n",
    "    axes[i].imshow(images[i].squeeze(), cmap=\"gray\")\n",
    "    axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network Model\n",
    "A **Neural Network (NN)** is a computational model inspired by the structure and functioning of biological neural networks in the human brain. It is designed to recognize patterns, approximate functions, and solve problems in various domains, such as computer vision, natural language processing, and predictive analytics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/neuron.jpg\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/neural_network.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of a Neural Network\n",
    "\n",
    "1. **Input Layer**:\n",
    "   - Accepts raw data or features as input.\n",
    "   - Each node in the input layer corresponds to a specific feature or data point.\n",
    "\n",
    "2. **Hidden Layers**:\n",
    "   - Composed of one or more layers between the input and output.\n",
    "   - Each layer consists of **neurons** (or nodes) connected to the neurons in the previous and subsequent layers.\n",
    "   - Performs computations using weighted inputs, biases, and an **activation function** to introduce non-linearity.\n",
    "\n",
    "3. **Output Layer**:\n",
    "   - Produces the final result of the neural network (e.g., a class label in classification tasks or a numerical value in regression tasks).\n",
    "   - The number of neurons depends on the problem (e.g., 10 neurons for a 10-class classification problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Pass\n",
    "A simple **feedforward neural network** using PyTorch's `torch.nn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer (784 -> 128)\n",
    "        self.fc2 = nn.Linear(128, 64)  # Hidden layer (128 -> 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # Output layer (64 -> 10)\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation for the output layer (logits)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define the Neural Network Class\n",
    "```python\n",
    "class NeuralNetwork(nn.Module):\n",
    "```\n",
    "- `NeuralNetwork` is a class that inherits from `nn.Module`, which is the base class for all neural networks in PyTorch.\n",
    "- By inheriting `nn.Module`, the class gains useful methods and functionality to define and train neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initialize the Network Layers\n",
    "```python\n",
    "def __init__(self):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.fc1 = nn.Linear(28 * 28, 128) \n",
    "    self.fc2 = nn.Linear(128, 64) \n",
    "    self.fc3 = nn.Linear(64, 10)\n",
    "    self.relu = nn.ReLU()\n",
    "```\n",
    "\n",
    "- **`__init__` Constructor**:\n",
    "  - Initializes the network layers and activation functions.\n",
    "  - `super(NeuralNetwork, self).__init__()` calls the `__init__` method of `nn.Module`, ensuring the base class is properly initialized.\n",
    "\n",
    "- **`nn.Linear(input_size, output_size)`**:\n",
    "  - Defines a fully connected (linear) layer:\n",
    "    - `input_size`: Number of input features.\n",
    "    - `output_size`: Number of output features.\n",
    "\n",
    "  Layers defined:\n",
    "  - `fc1`: First layer takes $28 \\times 28 = 784$ input features (flattened MNIST image) and outputs 128 features.\n",
    "  - `fc2`: Second layer takes 128 features and outputs 64 features.\n",
    "  - `fc3`: Third layer takes 64 features and outputs 10 features (corresponding to 10 digit classes).\n",
    "\n",
    "- **Activation Function**:\n",
    "  - `self.relu = nn.ReLU()`: Defines the Rectified Linear Unit (ReLU) activation function. This introduces non-linearity to the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the Forward Pass\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = x.view(-1, 28 * 28)  \n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.relu(self.fc2(x))\n",
    "    x = self.fc3(x) \n",
    "    return x\n",
    "```\n",
    "\n",
    "- **Forward Method**:\n",
    "  - Defines how the input flows through the network (the computation graph).\n",
    "  - Overridden from `nn.Module`.\n",
    "\n",
    "- **Step-by-Step Forward Pass**:\n",
    "  1. **Reshape Input**:\n",
    "     ```python\n",
    "     x = x.view(-1, 28 * 28)\n",
    "     ```\n",
    "     - Flattens the 2D image tensor of shape `(batch_size, 28, 28)` into a 1D tensor of shape `(batch_size, 784)`.\n",
    "     - `-1` infers the batch size dynamically.\n",
    "  \n",
    "  2. **First Layer**:\n",
    "     ```python\n",
    "     x = self.relu(self.fc1(x))\n",
    "     ```\n",
    "     - Passes the input through the first fully connected layer (`fc1`) with $784$ inputs and $128$ outputs.\n",
    "     - Applies the ReLU activation function.\n",
    "\n",
    "  3. **Second Layer**:\n",
    "     ```python\n",
    "     x = self.relu(self.fc2(x))\n",
    "     ```\n",
    "     - Passes the output of the first layer through the second fully connected layer (`fc2`) with $128$ inputs and $64$ outputs.\n",
    "     - Again, applies the ReLU activation function.\n",
    "\n",
    "  4. **Output Layer**:\n",
    "     ```python\n",
    "     x = self.fc3(x)\n",
    "     ```\n",
    "     - Passes the output of the second layer through the final layer (`fc3`) with $64$ inputs and $10$ outputs.\n",
    "     - No activation is applied here, as this is typically done outside the model (e.g., using `Softmax` or `CrossEntropyLoss`).\n",
    "\n",
    "  5. **Return Output**:\n",
    "     ```python\n",
    "     return x\n",
    "     ```\n",
    "     - Returns the raw output (logits) from the final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "\n",
    "1. **Structure**:\n",
    "   - This model has three layers:\n",
    "     - Input: $784 \\rightarrow 128$\n",
    "     - Hidden: $128 \\rightarrow 64$\n",
    "     - Output: $64 \\rightarrow 10$\n",
    "\n",
    "2. **Activation Functions**:\n",
    "   - ReLU is used for non-linearity in the hidden layers.\n",
    "\n",
    "3. **Forward Pass**:\n",
    "   - Defines how the input flows through the network.\n",
    "\n",
    "4. **Output**:\n",
    "   - The final output is a set of logits (raw scores) for each of the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How It Works with MNIST\n",
    "\n",
    "- **Input**: MNIST images are $28 \\times 28$ grayscale, represented as tensors of shape `(batch_size, 28, 28)`.\n",
    "- **Output**: The network produces logits for 10 digit classes (0-9).\n",
    "- **Training**: During training, the output logits are passed to a loss function like `nn.CrossEntropyLoss`, which applies softmax internally and computes the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model = NeuralNetwork()` creates an instance of the neural network, initializing its layers and activation functions.\n",
    "\n",
    "This `model` object is now ready for training, evaluation, and inference tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`nn.CrossEntropyLoss`** is a PyTorch loss function commonly used for **multi-class classification tasks**.\n",
    "- It computes the difference between the predicted class probabilities (logits) and the actual class labels.\n",
    "- The goal is to minimize this loss during training, helping the model improve its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input to `CrossEntropyLoss`:\n",
    "- **Predictions (logits)**:\n",
    "  - The model's output (raw scores) for each class.\n",
    "  - Shape: $(N, C)$, where $N$ is the batch size, and $C$ is the number of classes.\n",
    "  - These values are not probabilities but unnormalized scores (logits).\n",
    "- **Target (labels)**:\n",
    "  - The ground truth labels (class indices) for the samples in the batch.\n",
    "  - Shape: $(N,)$, where each value is an integer $0, 1, \\ldots, C-1$, representing the class index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations Inside `CrossEntropyLoss`:\n",
    "1. **Logits to Probabilities**:\n",
    "   - Applies the **Softmax** function to the logits to convert them into probabilities:\n",
    "     $$\n",
    "     p_i = \\frac{e^{z_i}}{\\sum_{j=1}^C e^{z_j}}\n",
    "     $$\n",
    "     where $z_i$ is the logit for class $i$, and $C$ is the number of classes.\n",
    "\n",
    "2. **Negative Log-Likelihood**:\n",
    "   - Uses the ground truth class labels to compute the **negative log-likelihood**:\n",
    "     $$\n",
    "     \\text{Loss} = - \\frac{1}{N} \\sum_{i=1}^N \\log(p_{i,y_i})\n",
    "     $$\n",
    "     where $y_i$ is the true class for sample $i$, and $p_{i,y_i}$ is the predicted probability for the correct class.\n",
    "\n",
    "   - This penalizes the model more heavily for assigning low probabilities to the correct class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Use `CrossEntropyLoss`?\n",
    "\n",
    "1. **Combines Softmax and Log-Loss**:\n",
    "   - Instead of manually applying Softmax and then computing the loss, `nn.CrossEntropyLoss` combines both steps for numerical stability and convenience.\n",
    "\n",
    "2. **Encourages Correct Class**:\n",
    "   - Minimizes the loss by assigning higher probabilities to the correct class.\n",
    "\n",
    "3. **Multi-Class Classification**:\n",
    "   - Specifically designed for problems where each sample belongs to one of $C$ classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Points\n",
    "\n",
    "- **Input Shape**:\n",
    "  - Logits: $(N, C)$\n",
    "  - Labels: $(N,)$\n",
    "\n",
    "- **Numerical Stability**:\n",
    "  - `nn.CrossEntropyLoss` directly works with logits, avoiding precision issues that may arise if you compute Softmax separately.\n",
    "\n",
    "- **Use Case**:\n",
    "  - Best suited for **classification tasks** where the output layer has one neuron per class, and the labels are class indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam** (short for Adaptive Moment Estimation) is an optimization algorithm used to update the model's weights during training. It is a widely used optimizer in deep learning because it combines the strengths of two popular methods: **Momentum** and **RMSProp**.\n",
    "\n",
    "`Adam` Class:\n",
    "- `Adam` is a PyTorch implementation of the Adam optimization algorithm.\n",
    "- It is used to compute and apply gradients during backpropagation.\n",
    "\n",
    "`model.parameters()`:\n",
    "- **Purpose**: Specifies the parameters (weights and biases) of the `model` that the optimizer should update.\n",
    "- `model.parameters()` returns an iterable of all the parameters in the neural network defined by the `NeuralNetwork` class, such as:\n",
    "  - `fc1.weight`, `fc1.bias`\n",
    "  - `fc2.weight`, `fc2.bias`\n",
    "  - `fc3.weight`, `fc3.bias`\n",
    "\n",
    "`lr=0.001`:\n",
    "- **Learning Rate (`lr`)**:\n",
    "  - A hyperparameter that controls the step size at each iteration when updating the weights.\n",
    "  - Smaller values make the training process slower but more stable, while larger values may speed it up but risk overshooting the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Adam Works\n",
    "\n",
    "The Adam optimizer updates each parameter in the model using this rule:\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta_t$: Parameter at step $t$.\n",
    "- $\\eta$: Learning rate (`lr`).\n",
    "- $\\hat{m}_t$: Biased-corrected **first moment estimate** (mean of gradients).\n",
    "- $\\hat{v}_t$: Biased-corrected **second moment estimate** (variance of gradients).\n",
    "- $\\epsilon$: Small constant for numerical stability (default: $10^{-8}$).\n",
    "\n",
    "#### Steps in Adam:\n",
    "1. Compute the gradient of the loss function with respect to each parameter.\n",
    "2. Update the first moment estimate $m_t$ (exponentially decaying average of past gradients).\n",
    "3. Update the second moment estimate $v_t$ (exponentially decaying average of squared gradients).\n",
    "4. Bias-correct $m_t$ and $v_t$ to avoid initialization issues.\n",
    "5. Update the parameters using the corrected values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    # Validation loss (optional for simplicity)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Initialization**\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "```\n",
    "- **Purpose**: Loops through a predefined number of training epochs.\n",
    "- **`epochs`**: The number of times the model will iterate over the entire training dataset.\n",
    "\n",
    " \n",
    "\n",
    "#### **2. Training Phase**\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "```\n",
    "- **Purpose**: Sets the model to training mode.\n",
    "  - Enables behaviors like dropout (if present) and batch normalization updates.\n",
    "\n",
    "##### **a. Initialize Training Loss**\n",
    "```python\n",
    "train_loss = 0\n",
    "```\n",
    "- **Purpose**: Resets the cumulative training loss for the current epoch.\n",
    "\n",
    "##### **b. Loop Over Batches**\n",
    "```python\n",
    "for images, labels in train_loader:\n",
    "```\n",
    "- **Purpose**: Iterates through batches of the training data (`images` and corresponding `labels`).\n",
    "- **`train_loader`**: A `DataLoader` object that handles loading and batching the training data.\n",
    "\n",
    "##### **c. Zero Out Gradients**\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "- **Purpose**: Clears the gradients of all trainable parameters.\n",
    "- Without this, gradients from previous steps would accumulate during backpropagation.\n",
    "\n",
    "##### **d. Forward Pass**\n",
    "```python\n",
    "outputs = model(images)\n",
    "```\n",
    "- **Purpose**: Passes the input `images` through the model to produce predictions (`outputs`).\n",
    "\n",
    "##### **e. Compute Loss**\n",
    "```python\n",
    "loss = criterion(outputs, labels)\n",
    "```\n",
    "- **Purpose**: Calculates the loss between the predicted `outputs` and actual `labels`.\n",
    "- **`criterion`**: The loss function (e.g., `CrossEntropyLoss`).\n",
    "\n",
    "##### **f. Backward Pass**\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "- **Purpose**: Computes the gradients of the loss with respect to the model’s parameters using backpropagation.\n",
    "\n",
    "##### **g. Update Parameters**\n",
    "```python\n",
    "optimizer.step()\n",
    "```\n",
    "- **Purpose**: Updates the model’s parameters based on the computed gradients and the optimizer's rules (e.g., Adam, SGD).\n",
    "\n",
    "##### **h. Accumulate Training Loss**\n",
    "```python\n",
    "train_loss += loss.item()\n",
    "```\n",
    "- **Purpose**: Adds the scalar value of the current batch loss to the cumulative training loss for the epoch.\n",
    "- **`loss.item()`**: Converts the tensor loss to a Python float.\n",
    "\n",
    "##### **i. Store Epoch Training Loss**\n",
    "```python\n",
    "train_losses.append(train_loss / len(train_loader))\n",
    "```\n",
    "- **Purpose**: Calculates and stores the average training loss for the epoch.\n",
    "\n",
    " \n",
    "\n",
    "#### **3. Validation Phase (Optional)**\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "- **Purpose**: Sets the model to evaluation mode.\n",
    "  - Disables behaviors like dropout and stops batch normalization updates.\n",
    "\n",
    "##### **a. No Gradient Computation**\n",
    "```python\n",
    "with torch.no_grad():\n",
    "```\n",
    "- **Purpose**: Ensures that no gradients are calculated during validation, saving memory and computational overhead.\n",
    "\n",
    "##### **b. Validation Loss Calculation**\n",
    "```python\n",
    "for images, labels in test_loader:\n",
    "    outputs = model(images)\n",
    "    val_loss += criterion(outputs, labels).item()\n",
    "```\n",
    "- **Purpose**:\n",
    "  - Loops through the validation data (`test_loader`) to compute the loss for each batch.\n",
    "  - Adds the scalar batch loss to the cumulative validation loss for the epoch.\n",
    "\n",
    "##### **c. Store Epoch Validation Loss**\n",
    "```python\n",
    "val_losses.append(val_loss / len(test_loader))\n",
    "```\n",
    "- **Purpose**: Calculates and stores the average validation loss for the epoch.\n",
    "\n",
    " \n",
    "\n",
    "#### **4. Log Progress**\n",
    "```python\n",
    "print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "```\n",
    "- **Purpose**: Prints the training and validation loss for the current epoch.\n",
    "- **Formatting**: Displays the loss values rounded to four decimal places.\n",
    "\n",
    " \n",
    "\n",
    "#### **Summary of the Workflow**\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - Passes batches of data through the model.\n",
    "   - Computes the loss, backpropagates, and updates the model’s parameters.\n",
    "   - Tracks the average training loss for the epoch.\n",
    "\n",
    "2. **Validation Phase**:\n",
    "   - Evaluates the model on the validation dataset without updating parameters.\n",
    "   - Tracks the average validation loss for the epoch.\n",
    "\n",
    "3. **Logs Losses**:\n",
    "   - Displays training and validation losses for each epoch.\n",
    "\n",
    "This structure ensures that the model is both trained and validated in each epoch, with clear logging to monitor performance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training and Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization example\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"mnist_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = NeuralNetwork()\n",
    "loaded_model.load_state_dict(torch.load(\"mnist_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "sample_image, sample_label = test_dataset[0]\n",
    "with torch.no_grad():\n",
    "    output = model(sample_image.unsqueeze(0))\n",
    "    _, prediction = torch.max(output, 1)\n",
    "print(f\"True Label: {sample_label}, Predicted Label: {prediction.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Advantages of Neural Networks**\n",
    "- Can model complex relationships in data.\n",
    "- Generalize well with sufficient training and regularization.\n",
    "- Widely applicable to diverse fields such as image recognition, speech processing, and medical diagnostics.\n",
    "\n",
    "### **Limitations of Neural Networks**\n",
    "- Requires a large amount of data and computational resources.\n",
    "- Training can be time-intensive.\n",
    "- Prone to overfitting if not properly regularized.\n",
    "\n",
    "In summary, neural networks are powerful tools for solving complex problems, with their success attributed to their ability to learn hierarchical representations of data. When designed and trained effectively, they can achieve remarkable performance across various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
