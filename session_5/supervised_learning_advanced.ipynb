{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cover.jpg\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Supervised Learning: Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Multi-class classification is a supervised learning task where the model needs to classify instances into one of three or more classes. Unlike binary classification, the output has more than two possible class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Flower Classification\n",
    "The classic Iris dataset is a perfect example of multi-class classification:\n",
    "- Input features: sepal length, sepal width, petal length, petal width\n",
    "- Output classes: Setosa, Versicolor, Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for better visualization\n",
    "iris_df = pd.DataFrame(X, columns=feature_names)\n",
    "iris_df[\"species\"] = pd.Categorical.from_codes(y, target_names)\n",
    "iris_df[\"target\"] = y\n",
    "# or\n",
    "# iris_df[\"species\"] = [target_names[y_] for y_ in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Shape:\", X.shape)\n",
    "print(\"\\nFeature Names:\", feature_names)\n",
    "print(\"\\nTarget Classes:\", target_names)\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(iris_df[\"species\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Support Vector Machines are powerful algorithms that create optimal hyperplanes to separate different classes. For multi-class problems, SVM uses either:\n",
    "- One-vs-One (OvO): Creates binary classifiers for each pair of classes\n",
    "- One-vs-Rest (OvR): Creates binary classifiers for each class against all others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/suport_vector_machine.png\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use SVM\n",
    "- Medium-sized datasets (up to ~10,000 samples)\n",
    "- High-dimensional data\n",
    "- Complex decision boundaries needed\n",
    "- When you need probability estimates\n",
    "- When you have non-linear relationships (using kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More on SVM](https://www.geeksforgeeks.org/support-vector-machine-algorithm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the SVM model\n",
    "svm_model = SVC(C=100, kernel=\"rbf\", gamma=\"auto\", random_state=42)\n",
    "\n",
    "\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(svm_model, X, y, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean accuracy:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` in Scikit-learn is a hyperparameter optimization technique that systematically works through multiple combinations of hyperparameter values to find the optimal configuration for a machine learning model. It is essentially a brute-force approach to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train SVM classifier\n",
    "model = SVC()\n",
    "\n",
    "# Perform grid search\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"kernel\": [\"sigmoid\", \"poly\", \"rbf\"],\n",
    "    \"gamma\": [\n",
    "        0.1,\n",
    "        0.2,\n",
    "        0.01,\n",
    "        \"scale\",\n",
    "        \"auto\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\"accuracy\", verbose=1)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(C=0.1, gamma=\"scale\", kernel=\"poly\", random_state=42)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(svm_model, X, y, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean accuracy:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=target_names,\n",
    "    yticklabels=target_names,\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix visualizes the performance of the model on three classes: \"setosa,\" \"versicolor,\" and \"virginica.\" Here's how to interpret each part of it:\n",
    "\n",
    "**True Positive Values (Diagonal)**:\n",
    "   - The values on the diagonal (from top left to bottom right) represent correctly classified instances for each class.\n",
    "   - **Setosa**: 23 instances were correctly classified as \"setosa.\"\n",
    "   - **Versicolor**: 19 instances were correctly classified as \"versicolor.\"\n",
    "   - **Virginica**: 17 instances were correctly classified as \"virginica.\"\n",
    "\n",
    "**False Positive and False Negative Values (Off-Diagonal)**:\n",
    "   - The off-diagonal values show misclassifications:\n",
    "     - 1 instance of \"versicolor\" was misclassified as \"virginica.\"\n",
    "     - There are no misclassifications for \"setosa\" or \"virginica\" in any other class.\n",
    "\n",
    "The model performed well on \"setosa\" and \"virginica,\" with no misclassifications in these categories. But there was a slight error in classifying \"versicolor,\" with one instance being misclassified as \"virginica.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "K-Nearest Neighbors is a simple but effective algorithm that classifies instances based on the majority class of their k nearest neighbors in the feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/k_nearest_neighbors.png\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use KNN\n",
    "- Small to medium-sized datasets\n",
    "- When you need a non-parametric model\n",
    "- When you have noisy training data\n",
    "- When you want an interpretable model\n",
    "- When computational resources during training are limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code evaluates the performance of a K-Nearest Neighbors (KNN) classifier for different values of $k$ to identify the optimal $k$ that maximizes accuracy.\n",
    "\n",
    "   - Iterates over a range of $k$ values.\n",
    "   - Trains and tests the model for each $k$.\n",
    "   - Stores the accuracy for each $k$ in a list.\n",
    "   - Determines and prints the $k$ with the highest accuracy.\n",
    "\n",
    "   - Plots the relationship between $k$ and the accuracy score to provide a visual representation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of odd values for the hyperparameter k (number of neighbors)\n",
    "k_range = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred = knn_model.predict(X_test)\n",
    "    k_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "optimal_k = k_range[np.argmax(k_scores)]\n",
    "print(f\"Optimal k value: {optimal_k}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, k_scores, \"bo-\")\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.title(\"Accuracy vs K Value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we use cross-validation to assess and optimize a key hyperparameter $k$ for the KNN algorithm, providing a more reliable evaluation compared to a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(knn_model, X, y, cv=5, scoring=\"accuracy\")\n",
    "    cv_score = np.mean(cv_scores)\n",
    "    k_scores.append(cv_score)\n",
    "\n",
    "optimal_k = k_range[np.argmax(k_scores)]\n",
    "print(f\"Optimal k value: {optimal_k}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, k_scores, \"bo-\")\n",
    "plt.xlabel(\"K Value\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.title(\"Accuracy vs K Value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Grid Search to fine best model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train KNN classifier\n",
    "pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier())])\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": k_range,\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    \"knn__metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=17, weights=\"distance\", metric=\"euclidean\")\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs excellently overall, with:\n",
    "\n",
    "- Perfect classification of setosa\n",
    "- Slight confusion between versicolor and virginica\n",
    "- High overall accuracy at 97%\n",
    "\n",
    "The slightly lower recall for versicolor (0.90) suggests that 1 out of 10 versicolor samples was misclassified as virginica, while the lower precision for virginica (0.91) indicates that one versicolor was incorrectly classified as virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison and Best Practices\n",
    "\n",
    "### 1. Algorithm Selection Guidelines\n",
    "- **SVM**:\n",
    "  - Better for complex decision boundaries\n",
    "  - Works well with high-dimensional data\n",
    "  - Requires careful parameter tuning\n",
    "  - More computationally intensive\n",
    "\n",
    "- **KNN**:\n",
    "  - Simple and interpretable\n",
    "  - Works well with low-dimensional data\n",
    "  - Sensitive to feature scaling\n",
    "  - Requires less parameter tuning\n",
    "  - Memory-intensive during prediction\n",
    "\n",
    "### 2. Cross-validation Best Practices\n",
    "1. Use stratified k-fold for imbalanced classes\n",
    "2. Choose appropriate number of folds (5-10 typically)\n",
    "3. Consider computational resources\n",
    "4. Use multiple metrics for evaluation\n",
    "\n",
    "### 3. Hyperparameter Tuning Tips\n",
    "1. Start with broad parameter ranges\n",
    "2. Use RandomizedSearchCV for initial search\n",
    "3. Refine with GridSearchCV in promising regions\n",
    "4. Consider trade-offs between performance and complexity\n",
    "\n",
    "### 4. Feature Engineering Considerations\n",
    "1. Scale features appropriately\n",
    "2. Handle categorical variables\n",
    "3. Consider dimensionality reduction\n",
    "4. Remove or handle outliers\n",
    "\n",
    "### 5. Model Evaluation Checklist\n",
    "1. Check for overfitting\n",
    "2. Examine confusion matrix\n",
    "3. Consider per-class performance\n",
    "4. Look at misclassified examples\n",
    "5. Validate on hold-out test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
