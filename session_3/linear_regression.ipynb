{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/cover.jpg\" alt=\"\" width=\"1920\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is a supervised learning algorithm that models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/linear_regression_graph.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression implementation using Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/salary_data.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`learning_rate = 0.01`\n",
    "This is the **learning rate $ \\alpha $**, which determines the size of the steps taken in the gradient descent algorithm. A learning rate of (0.01) means that each gradient step reduces the parameters slightly, ensuring controlled updates.\n",
    "\n",
    "`n_features = 1`\n",
    "This specifies the number of **features** (input variables) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_features = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weights = np.zeros(n_features)`\n",
    "This initializes the **weights** (coefficients) for the features to zeros. Since `n_features = 1`, this will create a 1D array with a single value of 0:\n",
    "\n",
    "`bias = 0`\n",
    "This initializes the **bias term** $ b $ to 0. The bias represents the intercept of the regression line when all feature values are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros(n_features)\n",
    "bias = 0\n",
    "\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.dot(X, weights)`\n",
    "  - Computes the **dot product** between the feature matrix $ X $ and the weights $ w $.\n",
    "  - If $ X $ has dimensions $ (m, n) $ (where $ m $ is the number of samples and $ n $ is the number of features), and $ weights $ has dimensions $ (n,) $, the result will be a vector of predicted values for each sample of size $ (m,) $.\n",
    "\n",
    "`+ bias`\n",
    "  - Adds the **bias** term $ b $ to each predicted value. Since $ bias $ is a scalar, it is broadcasted across all predictions.\n",
    "\n",
    "This corresponds to the linear regression equation:\n",
    "$$\n",
    "\\hat{y}_i = \\sum_{j=1}^n w_j x_{ij} + b\n",
    "$$\n",
    "for each sample $ i $ in the dataset, where:\n",
    "- $ \\hat{y}_i $: Predicted value for the $ i $-th sample\n",
    "- $ x_{ij} $: Value of the $ j $-th feature for the $ i $-th sample\n",
    "- $ w_j $: Weight for the $ j $-th feature\n",
    "- $ b $: Bias term\n",
    "\n",
    "In vectorized form for all $ m $ samples:\n",
    "$$\n",
    "\\hat{y} = X \\cdot w + b\n",
    "$$\n",
    "where:\n",
    "- $ X $: Feature matrix of size $ (m, n) $\n",
    "- $ w $: Weight vector of size $ (n,) $\n",
    "- $ b $: Scalar bias (added element-wise to the result of $ X \\cdot w $)\n",
    "\n",
    "This line computes the predicted outputs $ \\hat{y} $, which will later be compared with the actual values $ y $ to calculate the error and gradients during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.dot(X, weights) + bias\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color=\"blue\", label=\"Actual data\")\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`m = len(y)`\n",
    "- $ m $: Number of samples in the dataset.\n",
    "- This is used to normalize the gradient by dividing it by the total number of samples, ensuring that the updates to the weights and bias are appropriately scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(y)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dw = -(2 / m) * np.dot(X.T, (y - y_pred))`\n",
    "- Computes the gradient of the cost function with respect to the weights ($ w $).\n",
    "  \n",
    "##### **Mathematical Derivation**\n",
    "1. **Cost Function** (Mean Squared Error):\n",
    "   $$\n",
    "   J(w, b) = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i)^2\n",
    "   $$\n",
    "   where $ \\hat{y}_i = \\sum_{j=1}^n w_j x_{ij} + b $.\n",
    "\n",
    "2. **Gradient of $J(w, b)$ with respect to $ w_j $**:\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial w_j} = \\frac{2}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i) x_{ij}\n",
    "   $$\n",
    "\n",
    "3. **Vectorized Form**:\n",
    "   The gradient for all weights ($ w $) can be written as:\n",
    "   $$\n",
    "   dw = \\frac{2}{m} X^T (\\hat{y} - y)\n",
    "   $$\n",
    "   - $ X^T $: Transpose of the feature matrix $ X $, size $ (n, m) $.\n",
    "   - $ (\\hat{y} - y) $: Error vector, size $ (m, 1) $.\n",
    "   - $ dw $: Gradient vector for weights, size $ (n, 1) $.\n",
    "\n",
    "4. **Negative Sign**:\n",
    "   Gradient descent minimizes the cost function, so we take the negative of the gradient:\n",
    "   $$\n",
    "   dw = -\\frac{2}{m} X^T (y - \\hat{y})\n",
    "   $$\n",
    "\n",
    "**In Code**:\n",
    "- `X.T`: Transpose of the feature matrix $ X $.\n",
    "- `y - y_pred`: Difference (error) between actual values and predictions.\n",
    "\n",
    "$ dw $: Measures how much the cost function changes with respect to each weight. It considers the error ($ y - y_{pred} $) scaled by the corresponding feature values ($ X $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = -(2 / m) * np.dot(X.T, (y - y_pred))\n",
    "dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`db = -(2 / m) * np.sum(y - y_pred)`\n",
    "- Computes the gradient of the cost function with respect to the bias ($ b $).\n",
    "\n",
    "**Mathematical Derivation**\n",
    "1. **Gradient of $ J(w, b) $ with respect to $ b $**:\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial b} = \\frac{2}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i)\n",
    "   $$\n",
    "   In vectorized form:\n",
    "   $$\n",
    "   db = -\\frac{2}{m} \\sum_{i=1}^m (y - \\hat{y})\n",
    "   $$\n",
    "\n",
    "##### **In Code**:\n",
    "- `np.sum(y - y_pred)`: Computes the sum of errors across all samples.\n",
    "\n",
    "\n",
    "$ db $: Measures how much the cost function changes with respect to the bias. It considers the total error ($ y - y_{pred} $) across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = -(2 / m) * np.sum(y - y_pred)\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weights = weights - learning_rate * dw`\n",
    "- Updates the weights ($ w $) by subtracting the product of the learning rate ($ \\alpha $) and the gradient ($ dw $).\n",
    "  \n",
    "**Mathematical Context**:\n",
    "1. **Gradient Descent Formula for Weights**:\n",
    "   $$\n",
    "   w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}\n",
    "   $$\n",
    "   - $ w_j $: Current weight for feature $ j $.\n",
    "   - $ \\frac{\\partial J}{\\partial w_j} $: Gradient of the cost function with respect to $ w_j $ ($ dw $).\n",
    "   - $ \\alpha $: Learning rate, which determines the step size.\n",
    "\n",
    "2. **Vectorized Form**:\n",
    "   For all weights:\n",
    "   $$\n",
    "   w := w - \\alpha dw\n",
    "   $$\n",
    "   where $ dw $ is the gradient vector for all features.\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - The weights are adjusted in the opposite direction of the gradient.\n",
    "   - This ensures the cost function $ J(w, b) $ is minimized in successive iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights - learning_rate * dw\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bias = bias - learning_rate * db`\n",
    "- Updates the bias ($ b $) similarly to the weights.\n",
    "\n",
    "**Mathematical Context**:\n",
    "1. **Gradient Descent Formula for Bias**:\n",
    "   $$\n",
    "   b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "   $$\n",
    "   - $ b $: Current bias.\n",
    "   - $ \\frac{\\partial J}{\\partial b} $: Gradient of the cost function with respect to $ b $ ($ db $).\n",
    "   - $ \\alpha $: Learning rate.\n",
    "\n",
    "2. **Interpretation**:\n",
    "   - The bias is adjusted to reduce the error caused by the predictions not aligning with the actual data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = bias - learning_rate * db\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How It Works in Gradient Descent**\n",
    "1. The gradients $ dw $ and $ db $ point in the direction of steepest ascent of the cost function.\n",
    "2. Subtracting $ \\alpha dw $ and $ \\alpha db $ moves the parameters in the opposite direction, toward the minimum of the cost function.\n",
    "\n",
    "### **Effect of the Learning Rate**\n",
    "- $ \\alpha $: Controls the size of each update.\n",
    "  - If $ \\alpha $ is too small, convergence is slow.\n",
    "  - If $ \\alpha $ is too large, the updates might overshoot, and the algorithm may fail to converge.\n",
    "\n",
    "### **Iterative Updates**\n",
    "These updates occur iteratively over multiple epochs (or iterations). After enough updates, the weights and bias converge to values that minimize the cost function, yielding a linear model that best fits the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/salary_data.csv\")\n",
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "for x_, y_ in zip(X[:5], y[:5]):\n",
    "    print(f\"{x_} -> {y_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_features = 1\n",
    "\n",
    "weights = np.zeros(n_features)\n",
    "bias = 0\n",
    "\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.dot(X, weights) + bias\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color=\"blue\", label=\"Actual data\")\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"Predictions\")\n",
    "plt.scatter(X, y_pred, color=\"red\", label=\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = -(2 / m) * np.dot(X.T, (y - y_pred))\n",
    "db = -(2 / m) * np.sum(y - y_pred)\n",
    "\n",
    "dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights - learning_rate * dw\n",
    "bias = bias - learning_rate * db\n",
    "\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear Regression Animation](https://youtu.be/1hGsKphwC-A?si=uJqmTN6K7LadFmKh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End LinearRegression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate  # Step size for gradient descent\n",
    "        self.n_iterations = n_iterations  # Number of training iterations\n",
    "        self.weights = None  # Model weights (initialized later)\n",
    "        self.bias = None  # Model bias (initialized later)\n",
    "\n",
    "    def _initialize_parameters(self, n_features):\n",
    "        \"\"\"Initialize weights and bias to zeros\"\"\"\n",
    "        self.weights = np.zeros(n_features)  # Create array of zeros for weights\n",
    "        self.bias = 0  # Initialize bias to zero\n",
    "\n",
    "    def _compute_predictions(self, X):\n",
    "        \"\"\"Compute predictions using current weights and bias\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def _the_print(self, X, y, y_pred):\n",
    "        print(f\"X: {X}\")\n",
    "        print(f\"y: {y}\")\n",
    "        print(f\"y_pred: {y_pred}\")\n",
    "\n",
    "    def _compute_gradients(self, X, y_true, y_pred):\n",
    "        \"\"\"Compute gradients for weights and bias\"\"\"\n",
    "        m = len(y_true)  # Number of samples\n",
    "        dw = -(2 / m) * np.dot(X.T, (y_true - y_pred))  # Gradient for weights\n",
    "        db = -(2 / m) * np.sum(y_true - y_pred)  # Gradient for bias\n",
    "        return dw, db\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the model using gradient descent\"\"\"\n",
    "        n_features = X.shape[1]  # Get number of features\n",
    "        self._initialize_parameters(n_features)  # Initialize parameters\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            y_pred = self._compute_predictions(X)  # Forward pass by makeing predictions\n",
    "            dw, db = self._compute_gradients(X, y, y_pred)  # Compute gradients\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        return self._compute_predictions(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/salary_data.csv\")\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = LinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "# plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color=\"blue\", label=\"Actual data\")\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"Predictions\")\n",
    "plt.title(\"Linear Regression Example\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean((y - model.predict(X)) ** 2)\n",
    "print(f\"Loss: {loss} \\nRoot Mean Squared Error: {np.sqrt(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_experience = 4.5\n",
    "np.round(model.predict([[year_experience]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
