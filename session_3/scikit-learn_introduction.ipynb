{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-learn (sklearn)\n",
    "\n",
    "This notebook provides a comprehensive introduction to scikit-learn, covering its basic concepts, key features, and common usage patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Scikit-learn?\n",
    "Scikit-learn is Python's most popular machine learning library that provides:\n",
    "- Simple and efficient tools for data mining and data analysis\n",
    "- Accessible to everybody and reusable in various contexts\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "- Open source, commercially usable under BSD license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/scikit-learn-logo.png\" alt=\"ML Workflow Diagram\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's make sure we have all required packages installed. Run the following cell to import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, model_selection, datasets, metrics, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Concepts\n",
    "\n",
    "Scikit-learn is built on a few key concepts:\n",
    "\n",
    "1. **Estimators**: Objects that learn from data\n",
    "2. **Transformers**: Estimators that transform data\n",
    "3. **Predictors**: Estimators that make predictions\n",
    "\n",
    "Let's see these concepts in action with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 4, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a simple linear regression model\n",
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model.predict([[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color=\"blue\", label=\"Data points\")\n",
    "plt.plot(X, model.predict(X), color=\"red\", label=\"Linear regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Simple Linear Regression Example\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocessing\n",
    "\n",
    "Scikit-learn provides various tools for preprocessing data. Let's explore some common preprocessing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"feature1\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        \"feature2\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "        \"category\": [\"A\", \"C\", \"A\", \"D\", \"E\", \"B\", \"A\", \"F\", \"A\", \"B\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=data, x=\"feature1\", y=\"feature2\", hue=\"category\", style=\"category\", s=100\n",
    ")\n",
    "x_vals = data[\"feature1\"]\n",
    "y_vals = data[\"feature2\"]\n",
    "plt.plot(x_vals, y_vals, color=\"red\", linestyle=\"--\", label=\"y\")\n",
    "plt.title(\"Scatter Plot of Feature1 vs Feature2 by Category\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend(title=\"Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Standardization (Standard Scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StandardScaler` in scikit-learn standardizes features by removing the mean and scaling to unit variance. This means it transforms the data so that each feature has a mean of 0 and a standard deviation of 1. Standardization is particularly useful in machine learning when features have different units or ranges, as it ensures that each feature contributes equally to the model's learning process.\n",
    "\n",
    "Here's how `StandardScaler` works:\n",
    "- **Mean removal**: Centers the data around zero.\n",
    "- **Variance scaling**: Scales the data so that it has a standard deviation of 1.\n",
    "\n",
    "### Example\n",
    "If `X` is your data:\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "After scaling, `X_scaled` will have standardized values, which can improve the performance of algorithms sensitive to feature scales, such as gradient-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "numerical_features = [\"feature1\", \"feature2\"]\n",
    "scaled_data = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "scaled_data_df = pd.DataFrame(scaled_data, columns=numerical_features)\n",
    "scaled_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=scaled_data_df, x=\"feature1\", y=\"feature2\", s=100)\n",
    "x_vals = scaled_data_df[\"feature1\"]\n",
    "y_vals = scaled_data_df[\"feature2\"]\n",
    "plt.plot(x_vals, y_vals, color=\"red\", linestyle=\"--\", label=\"line\")\n",
    "plt.title(\"Scatter Plot of Feature1 vs Feature2 by Category\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend(title=\"Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.OneHotEncoder()\n",
    "encoded_categories = encoder.fit_transform(data[[\"category\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_categories.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_categories_df = pd.DataFrame(\n",
    "    encoded_categories.toarray(), columns=encoder.get_feature_names_out([\"category\"])\n",
    ")\n",
    "\n",
    "\n",
    "encoded_categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(encoded_categories_df, annot=True, cmap=\"YlGnBu\", cbar=True)\n",
    "plt.title(\"Category One-Hot Encoding Heatmap\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Selection and Evaluation\n",
    "\n",
    "Let's explore how to split data, train models, and evaluate their performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`datasets.make_classification`**: This function generates a synthetic dataset with:\n",
    "  - `n_samples=1000`: 1000 samples (data points).\n",
    "  - `n_features=4`: 4 features (variables or predictors).\n",
    "  - `n_classes=2`: 2 target classes (binary classification).\n",
    "  - `random_state=42`: Ensures reproducibility of the dataset generation by fixing the random seed.\n",
    "- **`X`**: Features of the dataset (input variables).\n",
    "- **`y`**: Target variable (labels or outcomes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic classification dataset\n",
    "X, y = datasets.make_classification(\n",
    "    n_samples=1000, n_features=4, n_classes=2, random_state=42\n",
    ")\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pd.DataFrame(X, columns=[\"x1\", \"x2\", \"x3\", \"x4\"])\n",
    "sample_data[\"y\"] = y\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot\n",
    "sns.pairplot(sample_data, hue=\"y\", diag_kind=\"hist\", corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`train_test_split`**: Splits the data into training and test sets.\n",
    "  - `X` and `y` are the input features and target labels, respectively.\n",
    "  - `test_size=0.2`: Allocates 20% of the data for testing and the remaining 80% for training.\n",
    "  - `random_state=42`: Ensures reproducibility of the split.\n",
    "\n",
    "  After this step:\n",
    "  - **`X_train`**: 80% of the feature data for training.\n",
    "  - **`X_test`**: 20% of the feature data for testing.\n",
    "  - **`y_train`**: 80% of the target labels for training.\n",
    "  - **`y_test`**: 20% of the target labels for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`LogisticRegression()`**: Creates an instance of the Logistic Regression model (a classifier).\n",
    "- **`fit(X_train, y_train)`**: Trains the model on the training data. The model learns the relationship between `X_train` (input features) and `y_train` (target labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`predict(X_test)`**: Uses the trained model (`clf`) to predict the target labels (`y_pred`) for the test data (`X_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_, y_ in zip(X_test[:5], y_test[:5]):\n",
    "    print(f\"Input: {x_} Actual: ({y_}) -> Prediction: {clf.predict([x_])[0]}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`metrics.classification_report`**: This function computes several classification metrics:\n",
    "  - **Precision**: The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "  - **Recall**: The ratio of correctly predicted positive observations to all actual positives.\n",
    "  - **F1-Score**: The harmonic mean of precision and recall.\n",
    "  - **Support**: The number of actual occurrences of each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`cross_val_score(clf, X, y, cv=5)`**: Performs k-fold cross-validation (with `cv=5`, i.e., 5 folds) to evaluate the model’s performance.\n",
    "  - The data (`X`, `y`) is split into 5 different subsets (folds), and for each fold, the model is trained on the other 4 folds and evaluated on the remaining fold.\n",
    "  - **`cv_scores`**: An array of scores (usually accuracy) from each fold.\n",
    "  \n",
    "- **`cv_scores.mean()`**: The average performance score across the 5 folds.\n",
    "- **`cv_scores.std()`**: The standard deviation of the cross-validation scores, which provides insight into the model's performance consistency.\n",
    "- **`cv_scores.std() * 2`**: Displays the \"approximate\" 95% confidence interval around the mean score, assuming a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "cv_scores = model_selection.cross_val_score(clf, X, y, cv=5)\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Average CV score: {cv_scores.mean():.3f} ± {cv_scores.std()*2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pipelines\n",
    "\n",
    "Pipelines are a powerful tool for combining multiple steps into a single estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "# Train and evaluate the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Pipeline Classification Report:\")\n",
    "print(metrics.classification_report(y_test, pipeline_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
